{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SnehaTanwar006/NLP/blob/main/Embedding(TF_IDF_BoW__Word2Vec).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4EwoDdEP3qMs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "faede31f-c3b2-4084-e168-3cf4455c4755"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (3.8.7)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.13)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.10)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (8.3.6)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.16.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.11.7)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.14.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.6.15)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.1)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.2)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Collecting en-core-web-lg==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.8.0/en_core_web_lg-3.8.0-py3-none-any.whl (400.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m400.7/400.7 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: en-core-web-lg\n",
            "Successfully installed en-core-web-lg-3.8.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_lg')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ],
      "source": [
        "!pip install scikit-learn spacy\n",
        "!python -m spacy download en_core_web_lg"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import spacy\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "JEaXvOw74mx5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Our three example \"movies\"\n",
        "corpus = [\n",
        "    \"The king fought the other king\",        # Movie 1: Semantically about royalty/power\n",
        "    \"The queen ruled the powerful realm\",    # Movie 2: Also about royalty/power\n",
        "    \"A dog ate the hot dog\"                  # Movie 3: Totally different topic\n",
        "]"
      ],
      "metadata": {
        "id": "XJMkidqi42r4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Method 1: Bag-of-Words (BoW) ---\n",
        "print(\"--- 1. Bag-of-Words (BoW) Analysis ---\")\n",
        "vectorizer_bow = CountVectorizer()\n",
        "bow_vectors = vectorizer_bow.fit_transform(corpus).toarray()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5-T5K43r5HaH",
        "outputId": "93a634f9-d904-4c9b-c62f-156e8156ac80"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- 1. Bag-of-Words (BoW) Analysis ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare Movie 1 (\"king\") vs Movie 2 (\"queen\")\n",
        "sim_bow_1_2 = cosine_similarity(bow_vectors[0:1], bow_vectors[1:2])\n",
        "print(f\"BoW Similarity (King vs Queen): {sim_bow_1_2[0][0]:.2f}\") # Expect this to be LOW"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Paae0Ri5NLf",
        "outputId": "e5255f46-90e0-4410-d8e8-89d8378d0092"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BoW Similarity (King vs Queen): 0.45\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare Movie 1 (\"king\") vs Movie 3 (\"dog\")\n",
        "sim_bow_1_3 = cosine_similarity(bow_vectors[0:1], bow_vectors[2:3])\n",
        "print(f\"BoW Similarity (King vs Dog):  {sim_bow_1_3[0][0]:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TAva0Wwp5jAn",
        "outputId": "a35d58f9-ed43-4b0d-96b5-6776fd32fc34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BoW Similarity (King vs Dog):  0.24\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Method 2: TF-IDF ---\n",
        "print(\"\\n--- 2. TF-IDF Analysis ---\")\n",
        "vectorizer_tfidf = TfidfVectorizer()\n",
        "tfidf_vectors = vectorizer_tfidf.fit_transform(corpus).toarray()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yYjkfxW55unv",
        "outputId": "22231806-5d09-4cb6-a923-284c9e809aa1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- 2. TF-IDF Analysis ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare Movie 1 (\"king\") vs Movie 2 (\"queen\")\n",
        "sim_tfidf_1_2 = cosine_similarity(tfidf_vectors[0:1], tfidf_vectors[1:2])\n",
        "print(f\"TF-IDF Similarity (King vs Queen): {sim_tfidf_1_2[0][0]:.2f}\") # Expect this to be LOW"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "79T9bZk65yHP",
        "outputId": "207ae627-7448-400c-c306-5e980faff9a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF-IDF Similarity (King vs Queen): 0.22\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare Movie 1 (\"king\") vs Movie 3 (\"dog\")\n",
        "sim_tfidf_1_3 = cosine_similarity(tfidf_vectors[0:1], tfidf_vectors[2:3])\n",
        "print(f\"TF-IDF Similarity (King vs Dog):  {sim_tfidf_1_3[0][0]:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xt22jesW523X",
        "outputId": "688692c2-a665-4da5-b340-4ae74a16c684"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF-IDF Similarity (King vs Dog):  0.10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Method 3: Word Embeddings (spaCy) ---\n",
        "print(\"\\n--- 3. Word Embedding Analysis ---\")\n",
        "nlp = spacy.load(\"en_core_web_lg\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4DEQ2fzI6DSh",
        "outputId": "f14ff63b-ec7f-4514-c36a-f5e5d0e504cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- 3. Word Embedding Analysis ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the document vectors (spaCy averages the word vectors)\n",
        "doc1 = nlp(corpus[0])\n",
        "doc2 = nlp(corpus[1])\n",
        "doc3 = nlp(corpus[2])"
      ],
      "metadata": {
        "id": "ZJfOiwW16GfR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare Movie 1 (\"king\") vs Movie 2 (\"queen\")\n",
        "sim_embed_1_2 = doc1.similarity(doc2)\n",
        "print(f\"Embedding Similarity (King vs Queen): {sim_embed_1_2:.2f}\") # Expect this to be HIGH"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WJz6JC1V6JIo",
        "outputId": "67d0c33c-d2be-4bcf-f202-12c1f073b01f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedding Similarity (King vs Queen): 0.84\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare Movie 1 (\"king\") vs Movie 3 (\"dog\")\n",
        "sim_embed_1_3 = doc1.similarity(doc3)\n",
        "print(f\"Embedding Similarity (King vs Dog):  {sim_embed_1_3:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U3okcLqz6Piv",
        "outputId": "3f6c2707-ec49-454b-f90f-e86b79389f1b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedding Similarity (King vs Dog):  0.60\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "qHCG3Agy6gWi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 637
        },
        "id": "f-383kz66gmf",
        "outputId": "56ca9600-b416-45f5-a67c-e00134093c87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gensim\n",
            "  Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n",
            "Collecting numpy<2.0,>=1.18.5 (from gensim)\n",
            "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scipy<1.14.0,>=1.7.0 (from gensim)\n",
            "  Downloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.1.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n",
            "Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.7/26.7 MB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m28.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.6/38.6 MB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy, scipy, gensim\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.15.3\n",
            "    Uninstalling scipy-1.15.3:\n",
            "      Successfully uninstalled scipy-1.15.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tsfresh 0.21.0 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.13.1 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed gensim-4.3.3 numpy-1.26.4 scipy-1.13.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy",
                  "scipy"
                ]
              },
              "id": "71a626f5cf224e339bd45a0be4be605d"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Import necessary libraries ---\n",
        "import string\n",
        "from gensim.models import Word2Vec"
      ],
      "metadata": {
        "id": "edI2S0R_6r-n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This is our custom corpus of text from a \"fantasy world\"\n",
        "fantasy_corpus_raw = [\n",
        "    \"The professor delivered a lecture on quantum physics in the main auditorium.\",\n",
        "    \"Students gathered in the library to prepare for the final examination.\",\n",
        "    \"The researcher published a groundbreaking paper in a renowned journal.\",\n",
        "    \"A group of scholars organized a conference on artificial intelligence.\",\n",
        "    \"The graduate student defended her thesis before the academic committee.\",\n",
        "    \"The university awarded scholarships to outstanding students.\",\n",
        "    \"The dean announced a new research grant for innovative projects.\",\n",
        "]\n"
      ],
      "metadata": {
        "id": "WLXBraEJ7Asf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Step 2: Preprocess the Text ---\n",
        "print(\"--- Step 2: Preprocessing Corpus ---\")\n",
        "processed_corpus = []\n",
        "for sentence in fantasy_corpus_raw:\n",
        "    # 1. Lowercase the sentence\n",
        "    lower_sentence = sentence.lower()\n",
        "\n",
        "    # 2. Remove punctuation\n",
        "    # This creates a translation table that maps each punctuation character to None\n",
        "    translator = str.maketrans('', '', string.punctuation)\n",
        "    no_punc_sentence = lower_sentence.translate(translator)\n",
        "\n",
        "    # 3. Tokenize (split into words)\n",
        "    tokens = no_punc_sentence.split()\n",
        "\n",
        "    processed_corpus.append(tokens)\n",
        "\n",
        "# Let's see what our processed corpus looks like\n",
        "print(\"Sample of processed corpus:\")\n",
        "print(processed_corpus[0])\n",
        "print(\"-\" * 20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eR2QEREa7Lkn",
        "outputId": "c1a93652-1d7e-4890-c26f-920f340611c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Step 2: Preprocessing Corpus ---\n",
            "Sample of processed corpus:\n",
            "['the', 'professor', 'delivered', 'a', 'lecture', 'on', 'quantum', 'physics', 'in', 'the', 'main', 'auditorium']\n",
            "--------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Step 3: Training the Word2Vec Model ---\n",
        "print(\"--- Step 3: Training the Custom Word2Vec Model ---\")\n",
        "\n",
        "# The key parameters for the model:\n",
        "# - sentences: Our processed corpus.\n",
        "# - vector_size: The number of dimensions for our word vectors.\n",
        "# - window: The maximum distance between the current and predicted word within a sentence.\n",
        "# - min_count: Ignores all words with a total frequency lower than this.\n",
        "# - workers: Number of CPU cores to use for training.\n",
        "model = Word2Vec(sentences=processed_corpus, vector_size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "print(\"Model training complete!\")\n",
        "print(f\"Vocabulary size: {len(model.wv.key_to_index)} words\")\n",
        "print(\"-\" * 20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5yeosqam7bAY",
        "outputId": "44c7cd30-aa44-4d9a-fa48-83498d8e74b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Step 3: Training the Custom Word2Vec Model ---\n",
            "Model training complete!\n",
            "Vocabulary size: 51 words\n",
            "--------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Step 4: Exploring Our Custom Model ---\n",
        "print(\"--- Step 4: Exploring the Model's Knowledge ---\")\n",
        "\n",
        "# Find the most similar words to \"student\"\n",
        "try:\n",
        "    similar_to_student = model.wv.most_similar('student', topn=3)\n",
        "    print(\"Words most similar to 'student':\", similar_to_student)\n",
        "except KeyError as e:\n",
        "    print(f\"Error: {e}\")\n",
        "\n",
        "# Find the most similar words to \"dean\"\n",
        "try:\n",
        "    similar_to_dean= model.wv.most_similar('dean', topn=3)\n",
        "    print(\"Words most similar to 'dean':\", similar_to_dean)\n",
        "except KeyError as e:\n",
        "    print(f\"Error: {e}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "eNqYS9y17qTf",
        "outputId": "4194a012-8ca2-401c-d8cc-364e84a862bc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Step 4: Exploring the Model's Knowledge ---\n",
            "Words most similar to 'student': [('groundbreaking', 0.2693989872932434), ('scholarships', 0.2060580551624298), ('dean', 0.1976974457502365)]\n",
            "Words most similar to 'dean': [('researcher', 0.23828670382499695), ('student', 0.1976974606513977), ('published', 0.1565902978181839)]\n"
          ]
        }
      ]
    }
  ]
}